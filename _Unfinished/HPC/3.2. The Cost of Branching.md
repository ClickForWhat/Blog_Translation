当 CPU 遇到条件跳转或任何其他类型的分支时，它不会只是闲坐着直到计算出它的条件是否被满足，而是通过推测，开始执行似乎更有可能即将执行的分支。在执行指令的时候，CPU 收集/计算每条指令所采取分支的统计信息，一段时间后，它们开始通过识别常见的模式来预测这些分支。

基于上述原因，分支真正的开销很大程度上取决于 CPU 的分支预测有多准。如果纯粹是投硬币那样 50% 的几率，你就得承受控制冒险，放弃整条流水线，再花费 15-20 个周期重来。如果某个分支总是被执行或从未被执行，除了检查条件外，你几乎不需要支付任何费用。

# 一个实验 - *An Experiment*

让我们通过一个实验来研究。我们将创建一个 0 到 99（含 99） 之间的随机整数数组：

```C++
for (int i = 0; i < N; i++)
    a[i] = rand() % 100;
```

之后我们创建一个循环，把小于 50 的元素累加起来。

```C++
volatile int s;

for (int i = 0; i < N; i++)
    if (a[i] < 50)
        s += a[i];
```

我们设置 $N=10^{6}$ ，然后运行多次以避免[冷缓存](https://en.algorithmica.org/hpc/cpu-cache/bandwidth)影响我们的结果。我们把累加变量声明为 `volatile` 来防止编译器对循环向量化、交叉迭代或以任何方式作弊。

在 Clang 编译器中，产生如下汇编代码：

```asm
    mov  rcx, -4000000
    jmp  body
counter:
    add  rcx, 4
    jz   finished   ; "jump if rcx became zero"
body:
    mov  edx, dword ptr [rcx + a + 4000000]
    cmp  edx, 49
    jg   counter
    add  dword ptr [rsp + 12], edx
    jmp  counter
```

我们的目标是模拟一个完全不可预测的分支，我们成功地实现了它：每个元素占用约 14 个 CPU 周期。我们可以粗略地估计发生了什么，我们假设分支的结果在 `<50` 和 `>=50` 中交替，并且每隔一次循环就会对流水线进行错误预测（50%），那么，每两次循环：

+ 我们刷新整个流水线，它在 Zen 2 上有 19 个周期（也就是说，它有 19 个阶段，每个阶段占用一个周期）。
+ 我们需要一次内存读取和一次比较，这大约需要 5 个周期。
+ 我们可以同时检查偶数和奇数迭代的条件，所以假设我们每 2 次迭代只有一次开销。
+ 在 `<` 分支的情况下，我们需要另外 4 个循环来将 `[i]` 加到 `volatile` 变量 `s`中。

那么，平均而言，每个元素需要花费 $(4+5+19)/2 = 14$ 个周期，和测量结果相匹配。

# 分支预测 - *Branch Prediction*

我们可以用一个可调整的参数 `P` 替换硬编码的 50，该参数 `P` 能设置 `<` 分支的概率：

```C++
for (int i = 0; i < N; i++)
    if (a[i] < P)
        s += a[i];
```

现在，如果我们对不同的 `P` 值进行基准测试，我们会得到一个有趣的图：

![[probabilities.png]]

正如我们设想的，在 50%-55% 的区间，分支预测错误的代价是最昂贵的。不过这个图是不对称的：因为只需要大约 1 个周期来检查条件并且它总不满足（P = 0），如果采取分支（P = 100），则总是需要大约 7 个周期来累加总和。

这张图不是单峰的：在 85%-90% 左右存在另一个局部最小值。考虑到我们需要执行更少的累加操作（相比 100% 概率而言），我们只在每个元素上花费 6.15 个循环，比我们总是执行累加快了 10-15%。在这一点上，分支预测错误不再大幅影响性能，因为当它发生时，不会丢弃整个指令缓冲区，而只会丢弃推测性调度的操作。从本质上讲，10-15% 的错误预测率是一个平衡点，在这个平衡点上，我们可以看到足够长的流水线不会停滞，但仍然可以节省 10-15%，选择更便宜的 `>=` 分支。

请注意，检查从未或几乎不会发生的情况几乎不需要花费任何费用。这就是为什么程序员如此频繁地使用运行时异常和基本情况检查的原因：如果它们确实很少见，那就不会花费任何成本。

# 模式检测 - *Pattern Detection*

在我们的示例中，高效分支预测所需的一切都来自硬件统计计数器。如果我们历史上使用分支 A 的次数比使用分支 B 的次数多，那么推测性地执行分支 A 是有意义的。但是现代 CPU 上的分支预测器比这先进得多，可以检测到更复杂的模式。

让我们将 `P` 固定回 50%，然后在求和循环之前先对数组进行排序：

```C++
for (int i = 0; i < N; i++)
    a[i] = rand() % 100;

std::sort(a, a + n);
```

我们仍然处理相同的元素，但顺序不同，它现在运行的周期不是 14 个，而是 4 个多一点，这正是纯 `<` 和 `>=` 分支的平均成本。

分支预测器可以检测到比“总是左，然后总是右”或“左-右-左-右”复杂得多的模式。如果我们只是将数组的大小 N 减小到 1000（不进行排序），那么分支预测器会记住整个比较序列，性能测试将测出开销在 4 个周期左右——事实上，甚至比数组被排序的情况还少一些，因为在前一种情况下，分支预测器需要花费一些时间在“总是是”和“总是否”状态之间切换。

# 提示分支的可能性 - *Hinting Likeliness of Branches*

如果你事先知道哪个分支更可能发生，那么将[该信息传递给编译器](https://en.algorithmica.org/hpc/compilation/situational)可能很有帮助：

```C++
for (int i = 0; i < N; i++)
    if (a[i] < P) [[likely]]
        s += a[i];
```

当 $P = 75$ 时，它测量每个元素大约 7.3 个周期，而没有提示的原始版本需要 8.3 个周期。

这个提示不会消除分支，也不会向分支预测器传递任何信息，但它会以一种方式改变[机器码布局](https://en.algorithmica.org/hpc/architecture/layout)，使 CPU 前端处理更可能的分支稍微快一些（尽管通常不超过一个周期）。

只有当你在编译阶段之前知道哪个分支更有可能被采用时，这种优化才有用。当分支从根本上不可预测时，我们可以尝试使用*Predication*（一种非常重要的技术，我们将在下一节中探讨）完全删除它。

# 致谢 - *Acknowledgements*

这个案例研究的灵感来自于[有史以来投票最多的 Stack Overflow 问题](https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array)。