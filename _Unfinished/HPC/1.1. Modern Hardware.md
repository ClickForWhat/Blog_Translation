在 1960 年代，微型计算机的主要缺点不是它们太慢——相对而言，肯定不是——真正的问题是它们太大了，而且难以使用，以及极其昂贵，只有世界超级大国的政府才能负担得起。它们的体积导致其昂贵的价格：它们需要大量定制组件，这些组件必须在宏观世界中由拥有电气工程高级学位的人非常仔细地组装，而且这个过程无法大规模生产。

发生改变的节点是*微芯片*的发展——单一、微小、完整的电路——它彻底改变了整个行业，甚至可能是 20 世纪最重要的创造。在 1965 年价值数百万美元的计算设备在 1975 年能装在一块 [4x4 毫米的硅片](https://en.wikipedia.org/wiki/MOS_Technology_6502)上[^1]，而现在你只需花 25 美元就能买到。史诗级的负担改善，使得家用电脑革命在接下来的十年里拉开了帷幕，像 Apple Ⅱ，雅达利 2600，Commodore 64，IBM PC 等电脑开始普及。

# 微芯片是怎么制造的 - *How Microchips are Made*

微芯片是通过称为[“光刻”](https://en.wikipedia.org/wiki/Photolithography)的一个过程，“印刷”在一片晶体硅上的，具体涉及：

1. 生长和切割一个[非常纯的硅晶体](https://en.wikipedia.org/wiki/Wafer_(electronics))，
2. 用一层[物质](https://en.wikipedia.org/wiki/Photoresist)覆盖它，当光子击中它时会溶解，
3. 用固定模式的光子撞击它，
4. 化学[蚀刻](https://en.wikipedia.org/wiki/Etching_(microfabrication))现在暴露的部分，
5. 去除剩余的光刻胶，

......然后在几个月内执行另外 40-50 个步骤来完成 CPU 的其余部分。

![[lithography.png]]

让我们现在重点关注“用光子撞击它”这个部分。为了达成这个目的，我们可以使用一种镜头系统，将图案投射到更小的区域，有效地制造出具有所有所需特性的微型电路。这样，20 世纪 70 年代的光学技术就能在一个指甲大小的尺寸上安装几千个晶体管，这给微芯片带来了宏观世界计算机所不具备的几个关键优势：

+ 更高的时钟速率（以前受光速限制）；
+ 规模化生产的能力；
+ 更少的材料和能源使用（对芯片而言），也就是每单位拥有更低的能耗。

除了这些直接的好处之外，光刻技术还为进一步提高性能提供了一条清晰的途径：你可以把镜头做得更好，这反过来又可以用相对较少的努力制造出更小，但功能相同的设备。

# 登纳德缩放定律 - *Dennard Scaling*

想想当我们把微芯片缩小会发生什么。更小的电路需要更少的材料，更小的晶体管需要更少的时间来切换（以及芯片中的所有其他物理过程），从而可以降低电压并提高时钟速率。

一个更仔细的观察，被称为*登纳德缩放定律*，指出当晶体管尺寸减少 30%：

 + 晶体管密度加倍（$0.7^2$ ≈ $0.5$）；
 + 时钟速率提升 40% （$1 \over 0.7$ ≈ $1.4$）；
 + 整体功率密度保持不变

由于单位制造成本是面积的函数，而开发成本主要是能源成本[^2]，每一代新产品的总成本应该大致相同，但时钟高 40%，晶体管数量翻倍，可以迅速使用，例如，增加新的指令或增加字长——以跟上存储器微芯片的小型化。

由于在设计过程中可以在能量和性能之间进行权衡，制程本身，如 “180nm” 或 “65nm”，直接可转化为晶体管的密度，成为 CPU 效率的标志[^3]。

从计算芯片历史上看，光学收缩是性能改进背后的主要驱动力。1975 年，英特尔前首席执行官戈登·摩尔（*Gordon Moore*）预测，微处理器中的晶体管数量将每两年翻一番。他的预言一直沿用至今，被称为*摩尔定律*。

![[dennard.png]]

登纳德缩放定律和摩尔定律都不是物理定律，而是聪明的工程师们的观察结果。由于基本的物理限制，它们都注定要在某个点停止，最终的限制是硅原子的大小。事实上，由于功率问题，登纳德缩放定律已经终止了。

从热力学角度来说，计算机是一种将电能转化为热能的高效设备。这些热量最终需要被移除，而从毫米级的晶体中释放的能量是有物理限制的。计算机工程师的目标是最大限度地提高性能，本质上只是试图选择最大可能的时钟频率，并让总体功耗保持不变。因此，在晶体管变小这件事上，如果它变得更小，它们的电容就更小，这意味着翻转它们所需的电压更小，这反过来又可以提高时钟速率。

听起来非常美好，但是在 2005 年至 2007 年左右，由于*泄漏效应*，这种策略停止了工作：电路特征已经变得极其之小，以至于它们的磁场开始使邻近电路中的电子朝着它们不应该的方向移动，导致不必要的加热和偶发的位翻转。

缓解这种情况的唯一方法是增加电压；但为了平衡增加电压带来的功耗提升，你不得不降低时钟频率，这反过来使整个过程的利润随着晶体管密度的增加而逐渐减少。在某种程度上，时钟速率无法再通过缩放来提高，小型化趋势开始放缓。

# 现代计算设备 - *Modern Computing*

登纳德缩放定律已经终结，但摩尔定律还没有消亡。

如今，时钟速率趋于稳定，但晶体管数量仍在增加，从而允许创建新的并行硬件。CPU 设计不再追求更快的周期，而是开始关注在单个周期内完成更多有用的事情。晶体管并没有变得更小，而是一直在改变形状。

这导致了越来越复杂的体系结构，能够在每个周期中处理数十、数百甚至数千种不同的事情。

![[die_shot.jpg]]
<center>AMD Zen CPU核心的模制图（约14亿个晶体管）</center>

以下是一些利用更多可用晶体管的核心方法，这些方法正在推动最近的计算机设计：

+ 重叠指令的执行，使 CPU 的不同部分保持忙碌（流水线）
+ 执行操作而不必等待前一个操作完成（推测性和乱序执行）
+ 添加多个执行单元来同时处理独立的操作（超标量处理器）
+ 增加机器字长，添加能够在分组的 128、256 或 512 位数据块上执行相同操作的指令（[SIMD](https://en.algorithmica.org/hpc/simd/)）
+ 在芯片上增加[缓存层](https://en.algorithmica.org/hpc/cpu-cache/)以加快 RAM 和外部存储器访问时间（存储器不完全遵循硅芯片的缩放定律）
+ 在一个芯片上添加多个相同的核心（并行计算，GPU）
+ 在主板上使用多个芯片，在数据中心使用多台更便宜的计算机（分布式计算）
+ 使用定制的硬件来解决特定的问题，更好地利用芯片（ASIC，FPGA）

对于现代计算机来说，通过“[对操作计数](https://en.algorithmica.org/hpc/complexity/)”来预测算法性能的方法已经不是小偏差，而是大错误了，它可能偏离了几个数量级。这需要新的计算模型和其他评估算法性能的方法。


[^1]: 由于电源管理、散热以及需要将其插入主板而不过度损坏，CPU的实际尺寸约为厘米级。
[^2]: 运行一个繁忙的服务器 2-3 年的电力成本大致相当于制造芯片本身的成本。
[^3]: 在某种程度上，当摩尔定律开始放缓时，芯片制造商实际上不再按照晶体管的大小来划分芯片制程，现在它更像是一个营销术语。一个特别委员会每两年召开一次会议，他们将之前的节点名称除以2的平方根，四舍五入到最接近的整数，宣布结果为新的节点名称，然后开始 party。"nm"实际不具有真实的纳米单位含义了。