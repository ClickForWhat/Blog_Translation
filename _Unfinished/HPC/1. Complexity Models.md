如果你曾经打开过计算机科学教科书，它可能会在最开始的某个地方介绍计算复杂性。简单地说，它是计算期间执行的基本操作（加法、乘法、读、写等等）的总数，可选地按其代价加权。

复杂性是一个古老的概念。它在 20 世纪 60 年代初被[系统地阐述](http://www.cs.albany.edu/~res/comp_complexity_ams_1965.pdf)，从那时起，它被普遍用作设计算法的成本函数。这个模型被迅速采用的原因是，它很好地近似了当时计算机的工作方式。

# 经典的复杂性理论 - *Classical Complexity Theory*

CPU 的“基本操作”被称为指令，指令的“开销”称为延迟。指令被存在*内存*中，由处理器一个接一个地执行，执行过程中还会有些内部的*状态*被存在*寄存器*里。其中的一个寄存器充当*指令指针*，它指出下一个需要读取和执行的指令的地址。每个指令会以某种方式改变处理器的状态（包括移动指令指针），可能修改主存，并且在开始下一个指令之前需要不同数量的 CPU 周期才能完成。

为了估计一个程序的真实运行时间，你需要把它所有执行的指令的延迟累加，并除以时钟频率。时钟频率即特定 CPU 每秒执行的周期数。

![[cpu.png]]

时钟频率是一个易变的且通常无法精确得知的参数，它取决于 CPU 模型、操作系统设置、当前微芯片的温度、其他部件的电源使用情况、以及其他的微小细节。相反，当以时钟周期表示时，指令延迟在不同 CPU 之间更为“静态”，甚至在某种程度上是一致的，因此，于分析而言，对它们进行计数更有用。

例如，定义上的矩阵乘法算法总共需要 $n^2⋅(n+n−1)$ 个算术操作：其中 $n^3$ 个用于乘法，$n^2⋅(n−1)$ 用于加法。如果我们去查这些指令的延迟（在一个称为指令表的特殊文档中，就像[这个](https://www.agner.org/optimize/instruction_tables.pdf)），我们就会发现，乘法花费 3 个周期，加法花费 1 个，因此我们需要 $3⋅n^3+n^2⋅(n-1)=4⋅n^3-n^2$ 个时钟周期来完成整个运算（我们暂时忽略了为这些指令提供正确数据所需要做的一切）。

类似于指令延迟的总和可以作为总执行时间的、与时钟无关的替代方法，计算复杂性可以用来量化抽象算法的内在时间需求，而不依赖于特定计算机的选择。

# 渐进复杂度 - *Asymptotic Complexity*

将执行时间表示为输入大小的函数的想法现在看起来很显然，但在 20 世纪 60 年代并非如此。当时，[典型的计算机](https://en.wikipedia.org/wiki/CDC_1604)耗资数百万美元，体积庞大，需要单独的房间，时钟频率以千赫兹为单位。它们被用于当时的实际任务，比如预测天气，发射火箭，或者计算苏联核导弹从古巴海岸能飞多远——所有这些都是有限长度的问题。那个时代的工程师主要关心的是如何乘以 3×3 的矩阵，而不是 n×n 的矩阵。

导致这种转变的原因是计算机科学家对计算机将在未来变得更快的信心。随着时间的推移，人们不再计算执行时间，不再计算周期，甚至不再精确地计算操作，取而代之的是一个估计，在足够大的输入下，误差不超过一个常数因子。在*渐进复杂度*中，复杂的”$4⋅n^3-n^2$ 个操作“被简化成”$Θ(n^3)$“，单个操作和所有其他复杂的硬件的初始成本被隐藏在”大O“中。

![[complexity.jpg]]

我们使用渐近复杂性的原因是，它提供了简单性，同时仍然足够精确，可以在大型数据集上产生有关相对算法性能的有用结果。在计算机终将变得足够快，能够在合理的时间内处理任何足够大的输入的承诺下，渐近更快的算法在现实也总是更快，而不管隐藏的常数是什么。

但是这个承诺被证明是不正确的，至少在时钟速度和指令延迟方面是不正确的，在本章中，我们将尝试解释为什么以及如何处理它。