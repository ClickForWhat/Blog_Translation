如果你正在阅读这本书，那么在你的计算机科学之旅的某个地方，你第一次开始关心你的代码的效率。

我的经历发生在高中，当时我意识到制作网站和编写有用的程序并不能让你进入大学，于是我进入了令人兴奋的算法编程奥林匹克竞赛世界。我是一个不错的程序员，特别是对于一个高中生来说，但我从来没有真正想过我的代码执行要花多少时间。但突然间，问题开始变得重要起来：现在每个问题都有严格的时间限制。我开始计算我的操作。一秒钟究竟能做多少？

当时我对计算机体系结构了解不多，无法回答这个问题。不过我不需要正确答案——我只需要一个经验法则。我的思考过程是：“2-3 GHz 意味着每秒执行二十到三十亿指令，如果在一个简单循环里用数组元素做一些事情，我需要自增循环计数，检查终止条件，计算数组索引之类的东西，所以让我们为每个用到的指令增加 3-5 个指令的空间”，这样最后使用 $5⋅10^8$ 作为估计。这些陈述都不够正确，但是计算我的算法需要多少操作并将其除以这个数字对于我的用例来说是一个很好的经验法则。

当然，真正的答案要复杂得多，并且高度依赖于你所处理的“操作”类型。对于[指针跟踪](https://en.algorithmica.org/hpc/cpu-cache/latency)之类的事情，它可以低至 $10^7$，对于 [SIMD 加速的线性代数](https://en.algorithmica.org/hpc/simd)，它可以高至 $10^{11}$。为了演示这些显著的差异，我们将使用不同语言实现的矩阵乘法的案例研究，并深入研究计算机如何执行它们。

# 语言的类型 - *Types of Languages*

在最低的层级上，计算机执行由二进制编码的指令组成的*机器码*，用于控制 CPU。它们既特定又古怪，并且需要花费大量的智力来处理，所以人们在创造计算机之后做的第一件事就是创造编程语言，它抽象了计算机如何操作的一些细节，以简化编程过程。

编程语言从根本上来说就是一个接口。用它编写的任何程序都只是一个更好的高级表示，在某种程度上仍然需要转换为机器码，以便在 CPU 上执行，有几种不同的方法可以做到这一点：

+ 从程序员的角度来看，有两种语言：编译型语言，在执行前进行预处理；解释型语言，在运行时使用称为解释器的单独程序执行。
+ 从计算机的角度来看，也有两种类型的语言：本机语言，直接执行机器码；托管语言，依赖于某种运行时来完成。

由于在解释器中运行机器代码没有意义，因此总共有三种类型的语言：

+ 解释型语言，例如 Python，JavaScript 或者 Ruby。
+ 带运行时的编译型语言，例如 Java，C# 或者 Erlang（以及在其虚拟机上工作的语言如 Scala，F# 或 Elixir）。
+ 原生（*native*）的编译型语言，例如 C，Go 或 Rust。

执行计算机程序没有“正确”的方法：每种方法都有自己的优点和缺点。解释器和虚拟机提供了灵活性，并支持一些不错的高级编程特性，如动态类型、运行时代码修改和自动内存管理，但这些特性带来了一些不可避免的性能权衡，我们现在将讨论这些权衡。

# 解释型语言 - *Interpreted languages*

以下是在 Python 中进行的标准的 1024x1024 大小的矩阵乘法：

```Python
import time
import random

n = 1024

a = [[random.random()
      for row in range(n)]
      for col in range(n)]

b = [[random.random()
      for row in range(n)]
      for col in range(n)]

c = [[0
      for row in range(n)]
      for col in range(n)]

start = time.time()

for i in range(n):
    for j in range(n):
        for k in range(n):
            c[i][j] += a[i][k] * b[k][j]

duration = time.time() - start
print(duration)
```

它花费了 630 秒。超过了 10 分钟！

让我们把这个数字带入我们先前的想法。假设运行它的 CPU 时钟频率是 1.4 GHz，意味着每秒执行 $1.4 · 10^9$ 个周期，整个计算差不多需要 $10^{15}$ 个周期，最内层的乘法每次大约需要 880 个周期。

如果考虑到 Python 得做一些事情来弄清楚程序员想干嘛，这就不足为奇了：

+ 它要解析表达式 `c[i][j] += a[i][k] * b[k][j]`。
+ 然后搞清楚 `a`，`b`，`c` 究竟是什么，并在一个存储类型信息的指定哈希表里寻找这些名字。
+ 当了解了 `a` 是一个列表后，获取 `[]` 运算符，检索指向 `a[i]` 的指针，发现它也是个列表，再获取 `[]` 运算符，获取 `a[i][k]` 的指针，最终得到元素。
+ 查找它的类型，发现它是 `float`，然后获取实现 `*` 运算符的方法。
+ 对 `b` 和 `c` 做同样的事，最后把结果与 `c[i][j]` 相加。

诚然，像 Python 这样广泛使用的语言，其解释器肯定经过了相当好的优化，对于重复执行的相同代码，会跳过其中一些步骤。不过，由于语言本身的设计，仍然有相当数量的开销无法避免。如果我们完全摆脱类型检查或者指针跟踪之类的东西，不管本地乘法的代价是多少，也许我们可以让每个乘法所占周期的比例接近 1？

# 托管语言 - *Managed Languages*

以下是相同的矩阵乘法，但是用 Java 实现：

```Java
import java.util.Random;

public class Matmul {
    static int n = 1024;
    static double[][] a = new double[n][n];
    static double[][] b = new double[n][n];
    static double[][] c = new double[n][n];
	
    public static void main(String[] args) {
        Random rand = new Random();
		
        for (int i = 0; i < n; i++) {
            for (int j = 0; j < n; j++) {
                a[i][j] = rand.nextDouble();
                b[i][j] = rand.nextDouble();
                c[i][j] = 0;
            }
        }
		
        long start = System.nanoTime();
		
        for (int i = 0; i < n; i++)
            for (int j = 0; j < n; j++)
                for (int k = 0; k < n; k++)
                    c[i][j] += a[i][k] * b[k][j];
                
        double diff = (System.nanoTime() - start) * 1e-9;
        System.out.println(diff);
    }
}
```

它现在10 秒内运行完毕 ，相当于每次乘法大约 13 个 CPU 周期，比 Python 快 63 倍。考虑到我们需要从内存中非顺序地读取 `b` 的元素，运行时间大致是预期的。

Java 需要编译，但并非*原生*语言。程序首先被编译成*字节码*，之后被一个虚拟机（*JVM*）解释。为了达到更高性能，那些常常被执行的那部分代码——例如内层的 `for` 循环——在运行时被编译成机器码，这样在执行时就不会有额外的开销。这种技术被称为*即时编译*（*JIT compilation）。

即时编译不是语言本身的特性，而是它的实现的特性。Python 也有使用即时编译的版本，称为 [PyPy](https://www.pypy.org/)，如果用它来执行此矩阵乘法，大概需要 12 秒。

# 编译型语言 - *Compiled Languages*

现在轮到 C 语言：

```C
#include <stdlib.h>
#include <stdio.h>
#include <time.h>

#define n 1024
double a[n][n], b[n][n], c[n][n];

int main() {
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < n; j++) {
            a[i][j] = (double) rand() / RAND_MAX;
            b[i][j] = (double) rand() / RAND_MAX;
        }
    }
	
    clock_t start = clock();
	
    for (int i = 0; i < n; i++)
        for (int j = 0; j < n; j++)
            for (int k = 0; k < n; k++)
                c[i][j] += a[i][k] * b[k][j];
	
    float seconds = (float) (clock() - start) / CLOCKS_PER_SEC;
    printf("%.4f\n", seconds);
    
    return 0;
}
```

运行需要花费 9 秒（使用 `gcc -O3` 编译它）。

这看起来并没有很大的提升——比使用即时编译的 Java 或 PyPy 只有 1 - 3秒的进步——然而我们还没有利用一个更好的 C 编译器生态系统。如果我们添加 `-march=native` 和 `-ffast-math` 标志，时间将迅速下降到 0.6 秒！

在这里，我们向[编译器传达](https://en.algorithmica.org/hpc/compilation/flags/)了我们正在运行的 CPU 的确切模型（`-march=native`），以及允许它自由地重新排布[浮点计算](https://en.algorithmica.org/hpc/arithmetic/float)（`-ffast-math`），因此它使用这些信息并使用[矢量化](https://en.algorithmica.org/hpc/simd)来实现这种加速。

这并不是说不可能在不对源代码进行重大更改的情况下，通过调优 PyPy 和 Java 的 JIT 编译器来实现相同的性能。但是对于直接编译为本机代码的语言来说，这当然更容易。

# BLAS - *BLAS*

最后，让我们看看专家优化的实现能够做些什么。我们将测试一个广泛使用的优化线性代数库，称为 [OpenBLAS](https://www.openblas.net/)。使用它最简单的方法是回到 Python，从 `numpy` 调用它：

```Python
import time
import numpy as np

n = 1024

a = np.random.rand(n, n)
b = np.random.rand(n, n)

start = time.time()

c = np.dot(a, b)

duration = time.time() - start
print(duration)
```

现在它只需要 0.12 秒：比自动矢量化的 C 版本加速了 5 倍，比我们最初的 Python 实现加速了 5250 倍！

你通常不会看到如此戏剧性的改善。现在，我们还没有准备好告诉你这一切是如何实现的。在 OpenBLAS 中，稠密矩阵的乘法实现里有[超过 5000 行手写的汇编代码](https://github.com/xianyi/OpenBLAS/blob/develop/kernel/x86_64/dgemm_kernel_16x2_haswell.S)，而且对*每种*体系结构单独定制。在之后的章节里，我们会一个个解释所有的相关技术，之后再返回到这个例子上，开发我们自己的 BLAS 级实现（只需要 40 行 C 代码！）。

# 附言 - *takeaway*

这里的关键教训是，使用本地的低级语言并不一定会给你带来性能；但它确实给了你对性能的控制权。

作为“每秒进行 N 次操作”这一简化说法的补充，许多程序员存在一个误解，就是使用不同的编程语言对这个数字有某种隐形的乘数。以这样的想法来[比较编程语言](https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html)的性能可能没有多大意义：编程语言从根本上说只是一种工具，它拿走一些对性能的控制，换取便捷的抽象。不管执行环境如何，利用硬件提供的机会在很大程度上仍然是程序员的工作。